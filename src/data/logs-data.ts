export const logs = [
  {
    "id": "chang",
    "name": "Chang",
    "title": "code_saturne, FAMIL",
    "github": "",
    "progress": {
      "solved": 5,
      "total": 7
    },
    "latest": "Day 9 & 10：Tune FAMIL",
    "lastUpdated": "2025-08-03T13:32:27.871Z",
    "fullText": "# Chang\r\ntitle: code_saturne, FAMIL\r\n\r\n## [Day 9 & 10] - [5/28 & 5/29]\r\n\r\n### ✅ 任務紀錄\r\n- Tune FAMIL\r\n\r\n### 🧪 設定資訊\r\n- flags =  ```-ftz -assume byterecl -O2 -i4 -r8 -zero -no-vec -align array64byte```\r\n- /path_to_famil 全部改掉\r\n\r\n---\r\n\r\n### 🐛 問題\r\n- 因為我們的 CPU 是 intel 的 xeon，但是因為是很舊的 x5670 所以他並不支援上課說過的 AVX，他最多只支援到 SSE4.2，所以優化時不能加上 AVX 系列的指令，他不知道那是甚麼\r\n- 在優化時發現怎麼優化都比 baseline 還慢，剛好心血來潮對了一下 baseline 的 f_run.log，這才很驚訝地發現 baseline 當初有幾個 module 的輸出是消失的，譬如說 UK 的臭氧資料，所以我這才重做了一份 baseline\r\n- famil 這個軟體有特別對自己的 rank 分配寫 script，所以如果硬要對 rank 做綁定或是特別的設定的話，讓 famil 變慢可以說是一定會發生的事情，因為綁錯 rank 會多出超多 comm time\r\n- -align array64byte 加入後獲得了很不錯的優化，在要進一步優化的時候發現，若要再進一步加上向量化的 flags 的時候，反而效果更差，我們認為是 -vec 會跟 -algin array64byte 在編譯上有某種未知的衝突 \r\n---\r\n\r\n### 🗂 Version\r\n- baseline (stat: 正常)\r\n![](image/chang/famil_new_baseline.png)\r\n- optimize result (stat: 正常)\r\n![](image/chang/famil_optimize.png)\r\n\r\n---\r\n\r\n### 📁 Log / Output\r\n---\r\n\r\n### 🖼 圖片（可選）\r\n\r\n## [Day 7 & 8] - [5/19 & 5/20]\r\n\r\n### ✅ 任務紀錄\r\n- 安裝 FAMIL\r\n\r\n### 🧪 設定資訊\r\n- intel oneapi mpi - 2021.15\r\n- intel compiler - 2023.2.4\r\n- hdf5-1.14.6\r\n- netcdf-c-4.9.2\r\n- netcdf-fortran-4.6.1\r\n\r\n---\r\n\r\n### 🐛 問題\r\n- 本軟體的 dependency 的問題超級嚴重，基本上就是漏了一個東西，你就會有幾百行的 error message 在你的 log 裡面\r\n- intel 版本獲取困難，intel 官網把 2023 以前的所有版本都刪除了，所以這個需要 ifort、icc 的軟體得用 key 連進去 intel 的 apt 紀錄庫，以此來獲得歷史版本\r\n- intel 的環境建置，他的環境在不同版本間有機會不相容，故我們最後選擇，intel 不支援舊版的指令前的最後一版 compiler 2023.2.4，並選取目前能安裝的最新版本 mpi 2021.15\r\n- config 的時候很容易會有前一次錯誤安裝的殘留，那些殘留的 .a, .so file 會污染我們的 config，系統會傾向直接去使用那些 compile 過的檔案，所以很容易跳出安裝的錯誤，在遇過這個問題之後，我安裝前都會先清掉上一次的殘留再行安裝\r\n- 因環境還有其他的題目與用到不同 mpi 的題目，故我把 famil 的環境設定檔全部寫成 script，在執行前呼叫即可設定 famil 的環境而不污染其他人的東西\r\n---\r\n\r\n### 🗂 Version\r\n- baseline (stat: 正常)\r\n![](image/chang/famil_baseline.png)\r\n\r\n---\r\n\r\n### 📁 Log / Output\r\n\r\n---\r\n\r\n### 🖼 圖片（可選）\r\n\r\n## [Day 5 & 6] - [5/27 & 5/28]\r\n\r\n### ✅ 任務紀錄\r\n- code_satrune 換成 intel mpi\r\n\r\n### 🧪 設定資訊\r\n- 把整個 code_saturne 用 intel mpi 重編\r\n\r\n---\r\n\r\n### 🐛 問題\r\n- 無，忽然想到我們電腦裡面還有另一組 mpi，不然來裝裝看好了，既然時間推不動了，然後也聽說不同家的 mpi 在通訊上也會有不同的設定，所以想說改改看\r\n---\r\n\r\n### 🗂 Version\r\n- optimize (stat: 正常)\r\n---\r\n\r\n### 📁 Log / Output\r\n![](image/chang/code_saturne_right_time.png)\r\n---\r\n\r\n### 🖼 圖片（可選）\r\n![](image/chang/close_set0.png)\r\n![](image/chang/close_set1.png)\r\n\r\n## [Day 3 & 4] - [5/12 & 5/13]\r\n\r\n### ✅ 任務紀錄\r\n- code_saturne openmp & mpi\r\n\r\n### 🧪 設定資訊\r\n- 把所有不必要的 log output 給關閉\r\n- 並實驗 openmp & mpi setup\r\n\r\n---\r\n\r\n### 🐛 問題\r\n- 無，只是一直都比 TA 的 baseline 還要慢一截\r\n---\r\n\r\n### 🗂 Version\r\n- optimize (stat: 正常)\r\n---\r\n\r\n### 📁 Log / Output\r\n![](image/chang/code_saturne_wrong_time.png)\r\n---\r\n\r\n### 🖼 圖片（可選）\r\n![](image/chang/close_set0.png)\r\n![](image/chang/close_set1.png)\r\n\r\n## [Day 1 & 2] - [5/12 & 5/13]\r\n\r\n### ✅ 任務紀錄\r\n- 安裝 code_saturne，並跑出 baseline\r\n---\r\n\r\n### 🧪 設定資訊\r\n- 如果要跑這個得話一定要用有支援 x11 forwarding 的環境去 ssh 因為我們的主機是沒有 $display 的\r\n- 運行前更改 run_solver 的 exec 的部分，將原本的 ./cs_solver 改成 mpirun -np 24 -host node1:12,node2:12 ./cs_solver\r\n- cpp 的副程式一定要擺在 case(AIR_QUALITY) 底下的 SRC 裡面才會被 code_saturne run --initialize 編譯到\r\n- baseline 的三隻副程式都在 Questions/code_saturne 裡面，想看的話請去那兒\r\n- setup gui 的圖片都在底下的圖片區\r\n---\r\n\r\n### 🐛 問題\r\n- 在剛開始要設定的時候就因為我是用 VS code 去做 ssh 的，但是 VS code 是沒有 x11 forwarding 的，花了一陣子才發現這個問題，改成 mobaXterm 就搞定了\r\n- 在剛開始裝的時候因為 setup 有的地方寫的不是很清楚，所以我把 cpp 的副程式放錯地方，看 listing 一直以為是我在 gui 的 setup 有問題，最後重新整個整理一遍才發現問題出在我檔案放錯資料夾\r\n---\r\n\r\n### 🗂 Version\r\n- baseline (stat: 正常)\r\n---\r\n\r\n### 📁 Log / Output\r\n![](image/chang/baseline.png)\r\n---\r\n\r\n### 🖼 圖片（可選）\r\n![](image/chang/MESH_Setup0.png)\r\n![](image/chang/MESH_Setup1.png)\r\n![](image/chang/MESH_Setup2.png)\r\n![](image/chang/Calc_Setup0.png)\r\n![](image/chang/Calc_Setup1.png)\r\n![](image/chang/Calc_Setup2.png)\r\n![](image/chang/Volume_Setup0.png)\r\n![](image/chang/Volume_Setup1.png)\r\n![](image/chang/Volume_Setup2.png)\r\n![](image/chang/Boundary_Setup0.png)\r\n![](image/chang/Boundary_Setup1.png)\r\n![](image/chang/Time_Setup.png)\r\n![](image/chang/Numerical_Setup.png)\r\n![](image/chang/Postprocessing_Setup0.png)\r\n![](image/chang/Postprocessing_Setup1.png)\r\n![](image/chang/Postprocessing_Setup2.png)\r\n![](image/chang/Postprocessing_Setup3.png)\r\n---"
  },
  {
    "id": "elystal",
    "name": "Elystal",
    "title": "HPL",
    "github": "",
    "progress": {
      "solved": 0,
      "total": 7
    },
    "latest": "FAMIL:",
    "lastUpdated": "2025-08-03T13:32:27.872Z",
    "fullText": "# Elystal\r\ntitle: HPL\r\n\r\n## [5/12] - [5/19]\r\n\r\n### ✅ 任務紀錄\r\nFAMIL:\r\n完成FAMIL的Build_dependencies和Run f_create_earth兩項工作\r\n\r\n### 🧪 設定資訊\r\n使用的是Openmpi，這也是我覺得問題所在\r\n\r\n### 🐛 問題\r\n在接續執行build時出現大量問題，我形容為「打地鼠」，解完一個多跳兩個\r\n指引建議我們使用Intel Mpi，然而我使用的是OpenMpi%gcc，這可能導致部分操作手續必須和指引不同，衍生各種問題\r\n所以當張凱淇同學接手時我建議他把我先前搞的東西全部砍掉從Intel開始做\r\n\r\n\r\n## [5/20]\r\n\r\n### ✅ 任務紀錄\r\n無\r\n\r\n### 🐛 問題\r\nMpi爆掉了\r\n我們打算使用Spack做控制，因為現在有些東西要Openmpi，有些要Intel mpi\r\n但我這邊成功裝好Spack上的Openmpi和cmake後，因為node2還老想著使用預設的Intel去跑所以會一直出問題\r\n而且現在好像連原本有裝的Openmpi（非Spack版本）也跟著出錯\r\n是相當折磨的一天\r\n\r\n\r\n## [5/25]\r\n\r\n### ✅ 任務紀錄\r\n今天，Mpi，堂堂復活\r\n又是張凱淇同學扛下了全部\r\n\r\n\r\n## [5/25] - [5/29]\r\n\r\n### ✅ 任務紀錄\r\nHPL:\r\n完成 \r\n過程中使用Excel建表找出最佳點\r\n\r\n### 🧪 設定資訊\r\nOpenBLAS，HPL-2.3，使用Openmpi\r\n\r\n\r\n## [5/30]\r\n\r\n### ✅ 任務紀錄\r\n無\r\n\r\n### 🐛 問題\r\n繳交截止前十幾分鐘才發現HPL的測試結果似乎遠不及應有的平均水準\r\n應該是我哪邊設定做得不對 但已經沒時間去修正他了\r\n大破防\r\n本來以為是Infiniband的問題 但指令開起來後數字還是一樣所以應該不是\r\n要找就得往回從OpenBLAS開始一步一步想出錯點 畢竟語言模型哪知道應該長怎樣\r\n可是接近段考我也很難湊時間出來找問題了\r\n"
  },
  {
    "id": "jc",
    "name": "JC",
    "title": "未指定任務",
    "github": "",
    "progress": {
      "solved": 5,
      "total": 7
    },
    "latest": "Day 1：成功完成 node1 的 setup",
    "lastUpdated": "2025-08-03T13:32:27.872Z",
    "fullText": "# JC\r\n\r\n## [Day 1] - [5/11]\r\n\r\n### ✅ 任務紀錄\r\n- 成功完成 node1 的 setup\r\n\r\n### 🧪 設定資訊\r\n- 使用 docker image 存取 BMC console\r\n- 開機時按 F12 導入 Ubuntu 安裝檔\r\n- 其餘部分照著 Week4 投影片做\r\n\r\n### 🐛 問題\r\n- 硬碟選擇的部分不確定有沒有選到 SSD\r\n\r\n### 🗂 Version\r\n\r\n### 📁 Log / Output\r\n\r\n### 🖼 圖片（可選）\r\n![](image/jc/node1_setup_success.png)\r\n![](image/jc/node1_storage.png)\r\n\r\n---\r\n\r\n## [Day 2] - [5/12]\r\n\r\n### ✅ 任務紀錄\r\n- 成功完成 node2 的 setup\r\n- 設定 InfiniBand\r\n    - ref: [比賽資訊 HackMD](https://hackmd.io/@J-Hsu/H1IaiS8gee?stext=755%3A455%3A0%3A1747803279%3A9b6vk1)\r\n- 設定兩台機器 ssh 互通\r\n- 設定 NFS 掛載\r\n\r\n### 🧪 設定資訊\r\n- node2 裝機流程與 node1 相同\r\n- node1 以 apt 安裝 `nfs-kernel-server`\r\n- node1 修改 `/etc/exports`\r\n    - ref: [week4 slide p.35](https://docs.google.com/presentation/d/1J_uiu7OeLwa8M2NMKNnp1Ie8-SlwvbCAPLDGmpLASYc/edit?usp=sharing)\r\n- node2 修改 `/etc/fstab`\r\n- 修改 `/etc/hosts` 讓 ssh 時不用打 IP address\r\n- 加入 node1 的 public key 到 node2 的 `~/.ssh/authorized_keys` 讓 node1 能夠連線到 node2\r\n- 加入我的 public key 到 node1 的 `~/.ssh/authorized_keys` 達成免密碼登入\r\n\r\n### 🐛 問題\r\n- 安裝 node2 的 OS 時發現有已被使用的硬碟\r\n- node2 OS 安裝完成後重開機沒有進入 Ubuntu\r\n    - 調整啟動順序 解決\r\n\r\n### 🗂 Version\r\n\r\n### 📁 Log / Output\r\n\r\n### 🖼 圖片（可選）\r\n![](image/jc/node2_storage.png)\r\n\r\n---\r\n\r\n## [Day 3] - [5/13]\r\n\r\n### ✅ 任務紀錄\r\n- 安裝 Gromacs\r\n- 嘗試執行 benchMEM\r\n\r\n### 🧪 設定資訊\r\n- Gromacs 安裝參考 week12 投影片 p.14\r\n\r\n### 🐛 問題\r\n- benchMEM 什麼參數都沒動，跑出的數據與助教 baseline 相差甚遠 (好很多)\r\n- 雖然有在 `.bashrc` 中 source Gromacs 的設定 script，甚至直接 export `gmx_mpi` 的 path，但用到 `gmx_mpi` 時，node2 會找不到這個指令\r\n    - 改用 `mpirun -np ... /opt/gromacs-2025.1-mpi/bin/gmx_mpi ...` 直接指定執行檔的絕對路徑才能成功執行\r\n\r\n### 🗂 Version\r\n- gromacs_benchMEM_default (stat: 待驗證)\r\n\r\n### 📁 Log / Output\r\n![](image/jc/benchMEM_v1.png)\r\n\r\n### 🖼 圖片（可選）\r\n\r\n---\r\n\r\n## [Day 4] - [5/19]\r\n\r\n### ✅ 任務紀錄\r\n- 重新嘗試執行 benchMEM，數據和助教 baseline 相近許多\r\n- 執行 benchRIB，數據和助教 baseline 相近\r\n- 檢查 hyperthreading 是否開啟\r\n- 參考網路資料嘗試優化 gromacs performance\r\n- 嘗試以 `gmx_mpi tune_pme` 工具找出最佳的 `-npme` 參數\r\n\r\n### 🧪 設定資訊\r\n\r\n### 🐛 問題\r\n- 不知道造成 performance 與上次執行結果不同的原因\r\n- 用以下指令執行 `gmx_mpi tune_pme` \r\n    ```bash\r\n    mpirun -np 24 --host node1:12,node2:12 \\ \r\n    /opt/gromacs-2025.1-mpi/bin/gmx_mpi tune_pme \\ \r\n    -mdrun /opt/gromacs-2025.1-mpi/bin/gmx_mpigmx_mpi \\ \r\n    -s benchMEM.tpr`\r\n    ```\r\n    會輸出\r\n    ```\r\n    Fatal error:\r\n    Cannot execute mdrun. Please check benchtest.log for problems!\r\n    ```\r\n    並在產生的 `benchtest.log` 中寫入\r\n    ```\r\n    mpirun: Error: unknown option \"-np\"\r\n    ```\r\n- 改用以下指令執行\r\n    ```bash\r\n    /opt/gromacs-2025.1-mpi/bin/gmx_mpi tune_pme \\\r\n    -mdrun /opt/gromacs-2025.1-mpi/bin/gmx_mpi \\\r\n    -np 24 --host node1:12,node2:12\\\r\n    -ntmpi 24 \\\r\n    -ntomp 1 \\\r\n    -s benchMEM.tpr\r\n    ```\r\n    結果輸出另一錯誤訊息: \r\n    ```\r\n    Fatal error:\r\n    Can't run multi-threaded MPI simulation yet!\r\n    ```\r\n\r\n### 🗂 Version\r\n- gromacs_optim_v1 (stat: 失敗)\r\n\r\n### 📁 Log / Output\r\n- benchMEM baseline\r\n    ![](image/jc/benchMEM_v2.png)\r\n- benchRIB baseline\r\n    ![](image/jc/benchRIB_v1.png)\r\n\r\n### 🖼 圖片（可選）\r\n\r\n---\r\n\r\n## [Day 5] - [5/20]\r\n\r\n### ✅ 任務紀錄\r\n- mpi 突然因為不明原因無法正常執行，嘗試修復\r\n\r\n### 🧪 設定資訊\r\n- 刪除原先以安裝檔安裝的 OpenMPI (`/opt/openmpi...`)\r\n- 重新嘗試以安裝檔手動安裝 OpenMPI，同樣錯誤仍然會出現\r\n\r\n### 🐛 問題\r\n- 透過手動安裝的 OpenMPI 仍會出現錯誤\r\n- 其他組員嘗試以 spack 安裝 OpenMPI，雖然可在單一 node 執行，但因為以 `mpirun` 使用其他 node 時不會自動 load openmpi ，導致另一 node 無法執行\r\n\r\n### 🗂 Version\r\n\r\n### 📁 Log / Output\r\n![](image/jc/mpi_error.png)\r\n\r\n### 🖼 圖片（可選）"
  }
]
